---
title: "0X-Model (TBU)"
author: "Team 48"
date: "2022-11-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Overview
The goal of this notebook is to test various model types on our cleaned data and assess the performance of each

-------------------------------------------------------------------------------

PART 1: BASIC DATA PREPARATION


Set the seed and read in required packages
```{r}
set.seed(1)
rm(list= ls())
library(stats)
library(dplyr)
library(glmnet)
library(caret)
library(randomForest)
library(xgboost)
```

Read in the data and remove unnecessary columns. Note there are still NAs in the dataset
```{r}
re <- read.csv("../Data/merged_data.csv")
re <- re[, !(colnames(re) %in% c('X', 'full_address', 'status', 'street', 'city', 'zip_code', 'sold_date', 'sold_year'))]
```


Scale the data - using dplyr package to quickly only scale numeric values
```{r}
re_scaled <- re %>%
   mutate_at(c(2:4,6:17), funs(c(scale(.))))
```

Split into training and test sets
```{r}
split <- floor(0.8 * nrow(re_scaled))
train_index <- sample(seq_len(nrow(re_scaled)), size = split)
train_re <- re_scaled[train_index,]
test_re <- re_scaled[-train_index,]
```

Check for PCA eligibility
```{r}
# if average correlation is >0.3 or <-0.3, use PCA
cor(re[, !(colnames(re_scaled) %in% c('state'))]) #looks pretty high in some features but I won't do PCA for now
```

Break columns up into listing features, tax features, and combined
```{r}
listing_features <- c('bed', 'bath', 'acre_lot', 'house_size')
tax_features <- c('n1_total', 'total_credit_amt', 'taxable_income_amt', 'mortgageint_amt', 'p_mortgageint_nr', 'inctax_amt', 'p_unemploy_nr', 'agi_amt', 'num_dependents', 'p_re_taxes_nr', 'agi_bucket')
combined_features <- c(listing_features, tax_features)
combined_features
```

-------------------------------------------------------------------------------

PART 2: LINEAR REGRESSION


Regress price off of the listing_features, tax_features, and combined_features
```{r}
lr_listing <- lm(reformulate(listing_features, 'price'), data = train_re)
summary(lr_listing)

lr_tax <- lm(reformulate(tax_features, 'price'), data = train_re)
summary(lr_tax)

lr_comb <- lm(reformulate(combined_features, 'price'), data = train_re)
summary(lr_comb)
```
Perform PCA using scaled data
```{r}
# throws an error if using all the data bc of the NAs
pca_re <- prcomp(re[,7:17], scale = TRUE)
pca_re
screeplot(pca_re, type = 'l')

var <- pca_re$sdev^2
propvar <- var/sum(var)

# Plot the proportion of variances from PC
plot(propvar, xlab = "Component", ylab = "Proportion of Variance Explained", ylim = c(0,1), type = "b")

# Plot the cumsum proportion of variances from PCA

cumsum(propvar)
plot(cumsum(propvar), xlab = "Principal Component", ylab = "Cumulative Prop. of Variance Explained",ylim = c(0,1), type = "b") #looks like 4 could be a good value to choose

# get the first 4 principal components
components <- pca_re$x[,1:4]
components

# create the PCA datq to be used next
re_pca_data <- cbind(re[,1], components)

```

Perform regression model on the PCA data
```{r}
#first need to make train and test sets for PCA data
pca_split <- floor(0.8 * nrow(re_pca_data))
pca_train_index <- sample(seq_len(nrow(re_pca_data)), size = pca_split)
pca_train_re <- re_pca_data[pca_train_index,]
pca_test_re <- re_pca_data[-pca_train_index,]

pca_model <- lm(V1~., data = as.data.frame(pca_train_re) )
summary(pca_model) #this is really bad but remember that none of the listing data is in this
```

Compute predictions on the test set and then compare the performance of each 
```{r}
lr_listing_preds <- lr_listing %>% predict(test_re)
lr_tax_preds <- lr_tax %>% predict(test_re)
lr_comb_preds <- lr_comb %>% predict(test_re)
lr_pca_preds <- pca_model %>% predict(as.data.frame(pca_test_re))

lr_listing_preds # NAs in the dataset are causing a lot of empty values
lr_tax_preds
lr_comb_preds #NAs in the dataset are causing a lot of empty values
lr_pca_preds

lr_listing_perf <- data.frame(R_squared = R2(lr_listing_preds, test_re$price), RMSE = RMSE(lr_listing_preds, test_re$price), MAE = MAE(lr_listing_preds, test_re$price))
lr_tax_perf <- data.frame(R_squared = R2(lr_tax_preds, test_re$price), RMSE = RMSE(lr_tax_preds, test_re$price), MAE = MAE(lr_tax_preds, test_re$price))
lr_combined_perf <- data.frame(R_squared = R2(lr_comb_preds, test_re$price), RMSE = RMSE(lr_comb_preds, test_re$price), MAE = MAE(lr_comb_preds, test_re$price))
lr_pca_perf <- data.frame(R_squared = R2(lr_pca_preds, test_re$price), RMSE = RMSE(lr_pca_preds, test_re$price), MAE = MAE(lr_pca_preds, test_re$price))

# NOTE: the NAs in the data are causing some problems here
lr_listing_perf
lr_tax_perf
lr_combined_perf
lr_pca_perf
```

-------------------------------------------------------------------------------

PART 3: LASSO REGRESSION


Build model
```{r}

predictors <- as.matrix(train_re[,7:17])
response <- as.matrix(train_re$price)

# building lasso
lasso = cv.glmnet(x= predictors, 
                  y = response,
                  alpha = 1,
                  nfolds = 5,
                  type.measure = "mse",
                  family = "gaussian"
                  )

#Output the [insert #] coefficients that lasso chose
coef(lasso, s=lasso$lambda.min)

#Fitting a new model with these significant variables
lr_lasso = lm(price ~ n1_total + total_credit_amt + taxable_income_amt + mortgageint_amt + p_mortgageint_nr + p_unemploy_nr + num_dependents + agi_bucket, data = train_re)
summary(lr_lasso)
```

Make predictions and evaluate performance
```{r}
lr_lasso_preds <- lr_lasso %>% predict(test_re)
lr_lasso_preds

lr_lasso_perf <- data.frame(R_squared = R2(lr_lasso_preds, test_re$price), RMSE = RMSE(lr_lasso_preds, test_re$price), MAE = MAE(lr_lasso_preds, test_re$price))

lr_lasso_perf
```

-------------------------------------------------------------------------------

PART 4: RANDOM FOREST


Build model
```{r}
rf_model <- randomForest(x = train_re[,7:17],
                         y = train_re[,1],
                         ntree = 128
                         )

rf_model
```

Make predictions and evaluate performance
```{r}
rf_preds <- predict(rf_model, test_re[,7:17])
rf_preds

rf_perf <- data.frame(R_squared = R2(rf_preds, test_re$price), RMSE = RMSE(rf_preds, test_re$price), MAE = MAE(rf_preds, test_re$price))
rf_perf
```

-------------------------------------------------------------------------------

PART 5: XGBOOST


Build model
```{r}
# train a model using our training data
xgb_model <- xgboost(data = predictors,
                     label = response,
                     max.depth = 3,
                     nrounds = 50
                     )
```

Make predictions and evaluate performance
```{r}
# generate predictions for our held-out testing data
xgb_preds <- predict(xgb_model, as.matrix(test_re[,7:17]))

xgb_perf <- data.frame(R_squared = R2(xgb_preds, test_re$price), RMSE = RMSE(xgb_preds, test_re$price), MAE = MAE(xgb_preds, test_re$price))
rf_perf
```








